{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12831845383859684815\n",
      "]\n",
      "Preprocessed data Loaded\n",
      "Embedding Matrix Loaded\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "if '__IPYTHON__' in globals():\n",
    "    ipython.magic('load_ext autoreload')\n",
    "    ipython.magic('autoreload 1')\n",
    "\n",
    "import helper\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, TimeDistributed, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import callbacks\n",
    "import collections\n",
    "#Verify access to the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#Load Data   \n",
    "preproc_source_sentences, preproc_target_sentences, source_tokenizer, target_tokenizer =\\\n",
    "helper.Pickle_out_data(\"Preprocessed_Data.pickle\")\n",
    "X_train, X_test, Y_train, Y_test=helper.Pickle_out_data(\"Split_Test_Train_Data.pickle\")\n",
    "print('Preprocessed data Loaded')\n",
    "embedding_matrix=helper.Pickle_out_data(\"embedded.pickle\")   \n",
    "print ('Embedding Matrix Loaded')\n",
    "max_source_sequence_length = preproc_source_sentences.shape[1]\n",
    "max_target_sequence_length = preproc_target_sentences.shape[1]\n",
    "source_vocab_size = len(source_tokenizer.word_index)+1\n",
    "target_vocab_size = len(target_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "1823250 English words.\n",
      "199 unique English words.\n",
      "\n",
      "1961295 French words.\n",
      "345 unique English words.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"I just loaded the dataset for giving you some information. \n",
    "you do not need to load that for training the model\n",
    "the dataset was already preprocessed and sevad as variable\"\"\"\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_sentences = load_data(source_path)\n",
    "target_sentences = load_data(target_path)\n",
    "print('Dataset Loaded')\n",
    "source_words_counter = collections.Counter([word for sentence in source_sentences for word in sentence.split()])\n",
    "target_words_counter = collections.Counter([word for sentence in target_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in source_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(source_vocab_size-1))\n",
    "\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in target_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(target_vocab_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 15, 100)           20000     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               120600    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 21, 200)           180600    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 512)           102912    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 346)           177498    \n",
      "=================================================================\n",
      "Total params: 601,610\n",
      "Trainable params: 581,610\n",
      "Non-trainable params: 20,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "def model_final(input_shape, output_sequence_length, s_size, t_size):\n",
    "    \"\"\"  \n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(s_size, 100, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:], weights=[embedding_matrix], trainable=False))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(100)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(t_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_final(preproc_source_sentences.shape,preproc_target_sentences.shape[1],\n",
    "                    len(source_tokenizer.word_index)+1,\n",
    "                    len(target_tokenizer.word_index)+1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CallBacks\n",
    "mfile = 'models/Glove_training_bach512.model.h5'\n",
    "model_checkpoint=callbacks.ModelCheckpoint(mfile, monitor='accuracy', save_best_only=True, save_weights_only=True)\n",
    "logger=callbacks.CSVLogger('results/training_bach_512.log')\n",
    "tensorboard=callbacks.TensorBoard(log_dir='results/training_bach_512')\n",
    "callbacks=[logger, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/keras-team/keras.git\n",
      "  Cloning https://github.com/keras-team/keras.git to c:\\users\\hadis\\appdata\\local\\temp\\pip-req-build-wqwmslvg\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: keras_applications>=1.0.6 in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: keras_preprocessing>=1.0.5 in c:\\users\\hadis\\anaconda3\\lib\\site-packages (from Keras==2.3.0) (1.1.0)\n",
      "Building wheels for collected packages: Keras\n",
      "  Building wheel for Keras (setup.py): started\n",
      "  Building wheel for Keras (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Hadis\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-rzb7nkh7\\wheels\\da\\a4\\7e\\6b7bd9af18cc2e23b8dd5ed6de07a7e13bd80a17214eb88932\n",
      "Successfully built Keras\n",
      "Installing collected packages: Keras\n",
      "  Found existing installation: Keras 2.3.0\n",
      "    Uninstalling Keras-2.3.0:\n",
      "      Successfully uninstalled Keras-2.3.0\n",
      "Successfully installed Keras-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/keras-team/keras.git 'C:\\Users\\Hadis\\AppData\\Local\\Temp\\pip-req-build-wqwmslvg'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "some keras version have problem in calculating val loss\n",
    "it will be most likely included in the next release 2.2.5.\n",
    "Until then, you can update to the HEAD of master from pip by doing:\n",
    "\"\"\"\n",
    "!Pip install git+https://github.com/keras-team/keras.git -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 7892."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(X_train, Y_train, batch_size=512, epochs=10, validation_split=0.01)\n",
    "\n",
    "#Save Model\n",
    "helper.save_model(model, 'models/Glove_training_bach_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run this for opening tensorboard on your Jupyter Notebook\n",
    "Yuo can also open that in the new browser.\"\"\"\n",
    "!pip install -q tf-nightly-2.0-preview\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11676), started 0:08:13 ago. (Use '!kill 11676' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d123b2f8b0570c48\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d123b2f8b0570c48\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime, os\n",
    "\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
